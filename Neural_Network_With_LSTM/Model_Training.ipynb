{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Dropout, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# from spacy.vocab import Vocab\n",
    "# from spacy.language import Language\n",
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making files\n",
    "from_lang='bengali'\n",
    "to_lang='english' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(from_lang):\n",
    "    \n",
    "    punc_table = str.maketrans({key: None for key in string.punctuation})\n",
    "    sentences = []\n",
    "    targets = []\n",
    "    count =0;\n",
    "    \n",
    "    with open('dataset/'+str(from_lang)+'_word_def.txt', 'r') as filee:\n",
    "        \n",
    "        for i, line in enumerate(filee):\n",
    "    #     for line in dataset:\n",
    "#             count+=1\n",
    "            words = line.strip('\\n').split('\\t')\n",
    "#             words = line\n",
    "            word = words[0]\n",
    "            definitions = words[1].split(';')\n",
    "            for definition in definitions:\n",
    "#                 print(definition)\n",
    "    # Removing data between () in definiton example to look at(something) => to look at\n",
    "                definition = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", definition).replace('  ', ' ')\n",
    "    # Removing the last extra space from the definition\n",
    "#                 if definition[-1] == ' ':\n",
    "#                     definition = definition[:-1]\n",
    "    # To lower case and removing punctuation\n",
    "                temp_word_list = definition.translate(punc_table).lower().split(' ')\n",
    "                temp_word_list = list(filter(None, temp_word_list))\n",
    "                mid_sent=['<start>'] + temp_word_list + ['<end>']\n",
    "                if mid_sent in sentences:\n",
    "                    continue\n",
    "                sentences.append(['<start>'] + temp_word_list + ['<end>'])\n",
    "\n",
    "                targets.append(word)\n",
    "    \n",
    "#     words = [word for word_sublist in sentences for word in word_sublist]\n",
    "    WORD=[]\n",
    "    for word_sublist in sentences:\n",
    "        for word in word_sublist:\n",
    "            WORD.append(word)\n",
    "    words=WORD\n",
    "    inf = float('inf')\n",
    "    frequency_dict = OrderedDict({'<end>': inf, '<start>': inf})\n",
    "    words_frequency_dict = sorted(Counter(words).most_common(None), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    defs_frequency_dict = sorted(Counter(targets).most_common(None), key=lambda x:x[1], reverse=True)\n",
    "    frequency_dict.update(words_frequency_dict)\n",
    "    frequency_dict.update(defs_frequency_dict)\n",
    "    frequency_dict.move_to_end('<start>', last=False)\n",
    "\n",
    "    word2idx = OrderedDict([(item[0], i) for i,item in enumerate(frequency_dict.items())])\n",
    "    idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "    \n",
    "    return sentences, targets, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2idx(sentences, targets, word2idx):\n",
    "    s = []\n",
    "    for sentence in sentences:\n",
    "        s.append([word2idx[word] for word in sentence])\n",
    "    t = []\n",
    "    for word in targets:\n",
    "        t.append(word2idx[word])\n",
    "    return s, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 9262\n",
      "Vocab Size: 17455\n",
      "Batches: 92\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 100\n",
    "\n",
    "sentences, targets, word2idx, idx2word = data(from_lang)\n",
    "sentences, targets = sent2idx(sentences, targets, word2idx)\n",
    "targets = np_utils.to_categorical(targets)\n",
    "batches = len(sentences) // BATCH_SIZE\n",
    "print('Sentences:',len(sentences))\n",
    "print('Vocab Size:',len(word2idx))\n",
    "print('Batches:',batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         1745500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, None, 100)         400       \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 17455)             2251695   \n",
      "=================================================================\n",
      "Total params: 4,115,355\n",
      "Trainable params: 4,114,899\n",
      "Non-trainable params: 456\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word2idx),100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(len(word2idx), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inputs, targets, batch_size):\n",
    "    inputs = np.array(inputs)\n",
    "    while True:\n",
    "        perm = np.random.permutation(len(inputs))[:batch_size]\n",
    "#         print(perm)\n",
    "        batch_inputs = pad_sequences(inputs[perm])\n",
    "        batch_targets = targets[perm]\n",
    "#         print(len(batch_targets[0]),len(targets[0]),)\n",
    "#         batch_targets = np_utils.to_categorical(batch_targets)\n",
    "#         print(type(batch_targets))\n",
    "#         print(len(batch_targets),len(batch_inputs))\n",
    "        yield batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "input_ = np.array(targets)\n",
    "# targets = input_.reshape(3281,5817, 1)\n",
    "print(len(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyani/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<generator..., epochs=100, steps_per_epoch=60)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/divyani/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "60/60 [==============================] - 12s 201ms/step - loss: 9.6122\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 9s 145ms/step - loss: 9.0319\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 8s 135ms/step - loss: 8.2028\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 9s 146ms/step - loss: 7.1610\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 10s 173ms/step - loss: 6.1264\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 9s 155ms/step - loss: 4.9351\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 10s 167ms/step - loss: 3.8398\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 9s 143ms/step - loss: 2.8944\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 9s 143ms/step - loss: 2.0673\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 8s 134ms/step - loss: 1.4285\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 9s 146ms/step - loss: 0.9381\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 9s 153ms/step - loss: 0.6390\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 9s 148ms/step - loss: 0.4057\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 10s 159ms/step - loss: 0.2805\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 9s 153ms/step - loss: 0.2018\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 9s 156ms/step - loss: 0.1231\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 9s 154ms/step - loss: 0.0877\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 10s 162ms/step - loss: 0.0701\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 8s 141ms/step - loss: 0.0480\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 8s 135ms/step - loss: 0.0407\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 8s 131ms/step - loss: 0.0334\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 8s 136ms/step - loss: 0.0292\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 8s 130ms/step - loss: 0.0245\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 9s 145ms/step - loss: 0.0214\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 9s 144ms/step - loss: 0.0175\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 9s 148ms/step - loss: 0.0155\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 8s 137ms/step - loss: 0.0155\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 8s 133ms/step - loss: 0.0137\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 8s 135ms/step - loss: 0.0129\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 8s 131ms/step - loss: 0.0136\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 9s 147ms/step - loss: 0.0118\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 8s 138ms/step - loss: 0.0115\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 8s 140ms/step - loss: 0.0092\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 9s 156ms/step - loss: 0.0087\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 9s 146ms/step - loss: 0.0087\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 8s 140ms/step - loss: 0.0074\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 10s 161ms/step - loss: 0.0070\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 9s 144ms/step - loss: 0.0067\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 9s 142ms/step - loss: 0.0064\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 9s 157ms/step - loss: 0.0058\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 10s 168ms/step - loss: 0.0055\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 9s 147ms/step - loss: 0.0055\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 9s 144ms/step - loss: 0.0054\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 9s 153ms/step - loss: 0.0053\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 8s 139ms/step - loss: 0.0047\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 9s 150ms/step - loss: 0.0042\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 9s 158ms/step - loss: 0.0040\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 8s 139ms/step - loss: 0.0039\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 9s 142ms/step - loss: 0.0036\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 8s 139ms/step - loss: 0.0035\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 10s 163ms/step - loss: 0.0032\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 8s 139ms/step - loss: 0.0031\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 9s 146ms/step - loss: 0.0030\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 9s 155ms/step - loss: 0.0029\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 9s 151ms/step - loss: 0.0028\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 9s 143ms/step - loss: 0.0026\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 9s 143ms/step - loss: 0.0025\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 10s 162ms/step - loss: 0.0024\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 9s 144ms/step - loss: 0.0023\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 9s 152ms/step - loss: 0.0022\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 9s 143ms/step - loss: 0.0021\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 9s 151ms/step - loss: 0.0020\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 9s 149ms/step - loss: 0.0019\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 10s 166ms/step - loss: 0.0019\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 9s 155ms/step - loss: 0.0017\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 8s 140ms/step - loss: 0.0017\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 9s 154ms/step - loss: 0.0017\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 9s 153ms/step - loss: 0.0016\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 9s 157ms/step - loss: 0.0015\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 9s 148ms/step - loss: 0.0015\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 10s 166ms/step - loss: 0.0014\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 11s 181ms/step - loss: 0.0013\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 18s 301ms/step - loss: 0.0013\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 9s 148ms/step - loss: 0.0013\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 9s 157ms/step - loss: 0.0012\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 9s 153ms/step - loss: 0.0012\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 9s 149ms/step - loss: 0.0011\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 10s 160ms/step - loss: 0.0011\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 9s 148ms/step - loss: 0.0010\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 9s 151ms/step - loss: 9.8500e-04\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 9s 148ms/step - loss: 9.5672e-04\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 9s 151ms/step - loss: 9.2756e-04\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 10s 162ms/step - loss: 8.9143e-04\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 8s 141ms/step - loss: 8.4974e-04\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 9s 145ms/step - loss: 8.2188e-04\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 10s 159ms/step - loss: 7.9960e-04\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 10s 163ms/step - loss: 7.6669e-04\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 9s 142ms/step - loss: 7.4713e-04\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 10s 161ms/step - loss: 7.0729e-04\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 9s 146ms/step - loss: 6.9220e-04\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 8s 142ms/step - loss: 6.7012e-04\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 10s 165ms/step - loss: 6.3608e-04\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 9s 151ms/step - loss: 6.1222e-04\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 9s 158ms/step - loss: 5.9472e-04\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 9s 151ms/step - loss: 5.7535e-04\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 9s 153ms/step - loss: 5.5277e-04\n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 8s 140ms/step - loss: 5.3784e-04\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 8s 138ms/step - loss: 5.2121e-04\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 9s 154ms/step - loss: 5.0173e-04\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 8s 137ms/step - loss: 4.8192e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7efc794bdc18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator = generator(sentences, targets, BATCH_SIZE), epochs = 100, samples_per_epoch = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/bengali.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/divyani/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1745500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 100)         400       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17455)             2251695   \n",
      "=================================================================\n",
      "Total params: 4,115,355\n",
      "Trainable params: 4,114,899\n",
      "Non-trainable params: 456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model/bengali.h5')\n",
    "# Check its architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TEST.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-1202bef88093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresult_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TEST.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilee\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TEST.txt'"
     ]
    }
   ],
   "source": [
    "test_label=[]\n",
    "result_label=[]\n",
    "with open('TEST.txt', 'r') as filee:\n",
    "    for i, line in enumerate(filee):\n",
    "        pair = line.strip('\\n').split('\\t')\n",
    "        word = pair[0]\n",
    "        test_label.append(word)\n",
    "        definition = pair[1]\n",
    "        words = definition.split(' ')\n",
    "        idxs = []\n",
    "        for word in words:\n",
    "#             print(word)\n",
    "            if(word in word2idx):\n",
    "                idxs.append(word2idx[word])\n",
    "\n",
    "        idxs = np.array([0] + idxs + [1]).reshape((1,len(idxs) + 2))\n",
    "        prediction = model.predict(idxs, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        meaning = idx2word[index]\n",
    "        result_label.append(meaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_label, result_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suiting\n"
     ]
    }
   ],
   "source": [
    "definition = 'a fabric used for suits'\n",
    "words = definition.split(' ')\n",
    "idxs = []\n",
    "for word in words:\n",
    "#             print(word)\n",
    "    if(word in word2idx):\n",
    "        idxs.append(word2idx[word])\n",
    "\n",
    "idxs = np.array([0] + idxs + [1]).reshape((1,len(idxs) + 2))\n",
    "prediction = model.predict(idxs, verbose=0)\n",
    "index = np.argmax(prediction)\n",
    "meaning = idx2word[index]\n",
    "print(meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
